{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa7a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading datasets and fairlearn\n",
    "\n",
    "!gdown 120Q64To3z6yqp6T9M-q365UUR4-55LyX\n",
    "\n",
    "!gdown 1dZT7wMzJqRPzNRSNpcBcpf59wEc33Nd3\n",
    "\n",
    "%pip install fairlearn\n",
    "\n",
    "# Model Training and Fairness evaluation Pre-intervention\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from fairlearn.metrics import MetricFrame, demographic_parity_difference, equalized_odds_difference\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data, train a model, and evaluate its\n",
    "    performance and fairness.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv('data.csv', sep=';', quotechar='\"')\n",
    "        print(\"DataFrame columns:\", df.columns) # Added this line to print column names\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'dataset.csv' not found. Make sure the dataset file is in the same directory.\")\n",
    "        return\n",
    "\n",
    "    #Data Preprocessing\n",
    "    X = df.drop('Target', axis=1)\n",
    "    y = df['Target']\n",
    "    # The list of sensitive features to evaluate\n",
    "    sensitive_features_list = ['Marital status', 'Application mode', 'Course',\n",
    "                                'Previous qualification', 'Nacionality',\n",
    "                               \"Mother's qualification\",\n",
    "                               'Educational special needs',\n",
    "                               'Tuition fees up to date', 'Gender',\n",
    "                               'Age at enrollment', 'International']\n",
    "\n",
    "\n",
    "    # Identify categorical and numerical features\n",
    "    # exclude sensitive features\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.difference(sensitive_features_list)\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.difference(sensitive_features_list)\n",
    "\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "    # Create a column transformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ], remainder='passthrough') #sensitive features in X for evaluation\n",
    "\n",
    "\n",
    "    #Model Training\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Training\n",
    "    print(\"Training the classification model...\")\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    print(\"Model training complete.\\n\")\n",
    "\n",
    "    #Model Evaluation\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "    print(\"--- Model Performance Evaluation ---\")\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Print the classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 35 + \"\\n\")\n",
    "\n",
    "\n",
    "    #Fairness Evaluation (One-vs-Rest)\n",
    "\n",
    "    unique_classes = y.unique()\n",
    "\n",
    "    for sensitive_feature in sensitive_features_list:\n",
    "        if sensitive_feature not in X_test.columns:\n",
    "            print(f\"Warning: Sensitive feature '{sensitive_feature}' not found in the test set. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        sensitive_test = X_test[sensitive_feature]\n",
    "\n",
    "        print(f\"\\n--- Fairness Evaluation (by '{sensitive_feature}' - One-vs-Rest) ---\")\n",
    "\n",
    "        for target_class in unique_classes:\n",
    "            print(f\"\\nEvaluating fairness for class: '{target_class}' (One-vs-Rest)\")\n",
    "\n",
    "            # Create binary target\n",
    "            y_test_binary = (y_test == target_class).astype(int)\n",
    "            y_pred_binary = (y_pred == target_class).astype(int)\n",
    "\n",
    "            #compute metrics grouped by the sensitive feature\n",
    "            metrics_binary = {\n",
    "                'accuracy': accuracy_score,\n",
    "                'precision': lambda y_true_b, y_pred_b: classification_report(y_true_b, y_pred_b, output_dict=True, zero_division=0)['weighted avg']['precision'],\n",
    "                'recall': lambda y_true_b, y_pred_b: classification_report(y_true_b, y_pred_b, output_dict=True, zero_division=0)['weighted avg']['recall']\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                grouped_on_sensitive = MetricFrame(metrics=metrics_binary,\n",
    "                                                       y_true=y_test_binary,\n",
    "                                                       y_pred=y_pred_binary,\n",
    "                                                       sensitive_features=sensitive_test)\n",
    "\n",
    "                print(f\"Metrics grouped by '{sensitive_feature}':\")\n",
    "                print(grouped_on_sensitive.by_group)\n",
    "\n",
    "                # Calculate and print fairness metrics\n",
    "                print(\"Fairness Metrics for this class:\")\n",
    "                dpd_binary = demographic_parity_difference(y_test_binary, y_pred_binary, sensitive_features=sensitive_test)\n",
    "                print(f\"Demographic Parity Difference: {dpd_binary:.4f}\")\n",
    "\n",
    "                eod_binary = equalized_odds_difference(y_test_binary, y_pred_binary, sensitive_features=sensitive_test)\n",
    "                print(f\"Equalized Odds Difference: {eod_binary:.4f}\")\n",
    "                print(\"-\" * 35)\n",
    "\n",
    "            except ValueError as e:\n",
    "                print(f\"Could not calculate fairness metrics for class '{target_class}' with sensitive feature '{sensitive_feature}': {e}\")\n",
    "                print(\"This might happen if one of the sensitive feature groups has no samples for this class in the test set.\")\n",
    "                print(\"-\" * 35)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# Visualisations to gauge biases in each feature\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('data.csv', sep=';', quotechar='\"')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'data.csv' not found. Make sure the dataset file is in the same directory.\")\n",
    "    exit() # Exit the program if the file is not found\n",
    "\n",
    "# Iterate through each column and create a frequency distribution plot\n",
    "for column in df.columns:\n",
    "    # Calculate the frequency distribution\n",
    "    freq_dist = df[column].value_counts().reset_index()\n",
    "    freq_dist.columns = [column, 'Frequency']\n",
    "\n",
    "    # Create the bar plot\n",
    "    fig = px.bar(freq_dist, x=column, y='Frequency', title=f'Frequency Distribution of {column}')\n",
    "\n",
    "    # Display the plot\n",
    "    fig.show()\n",
    "\n",
    "## Anova Analysis to compute most significant features\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Identify the target column\n",
    "target_column = 'Target'\n",
    "\n",
    "# Exclude the target column\n",
    "numerical_features = numerical_cols.drop(target_column, errors='ignore')\n",
    "\n",
    "\n",
    "# Identify unique classes\n",
    "unique_classes = df[target_column].unique()\n",
    "\n",
    "#store p-values\n",
    "anova_results = {}\n",
    "\n",
    "# Perform ANOVA\n",
    "for feature in numerical_features:\n",
    "    data_for_anova = []\n",
    "    for class_value in unique_classes:\n",
    "        feature_data = df[df[target_column] == class_value][feature].dropna()\n",
    "        if len(feature_data) >= 2:\n",
    "            data_for_anova.append(feature_data)\n",
    "\n",
    "    if len(data_for_anova) >= 2:\n",
    "        try:\n",
    "            f_statistic, p_value = stats.f_oneway(*data_for_anova)\n",
    "            anova_results[feature] = p_value\n",
    "        except ValueError as e:\n",
    "            print(f\"Could not perform ANOVA for feature '{feature}': {e}\")\n",
    "            print(\"This might happen if a group has constant values or insufficient variance.\")\n",
    "    else:\n",
    "        print(f\"Skipping ANOVA for feature '{feature}': Insufficient data in groups.\")\n",
    "\n",
    "# Print the p-values\n",
    "print(\"\\nANOVA Test p-values:\")\n",
    "for feature, p_value in anova_results.items():\n",
    "    print(f\"{feature}: {p_value:.4f}\")\n",
    "\n",
    "# Model Training and Fairness evaluation Post-intervention\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from fairlearn.metrics import MetricFrame, demographic_parity_difference, equalized_odds_difference\n",
    "\n",
    "# Define global variables\n",
    "global dpd_results_data1\n",
    "global eod_results_data1\n",
    "global dpd_results_data2\n",
    "global eod_results_data2\n",
    "\n",
    "def perform_fairness_evaluation(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Performs fairness evaluation on a given DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Performing fairness evaluation for: {dataset_name} ---\")\n",
    "\n",
    "    # Define features (X) and target (y)\n",
    "    # The target column is assumed to be 'Target'\n",
    "    X = df.drop('Target', axis=1)\n",
    "    y = df['Target']\n",
    "    if dataset_name == 'data2 - Sheet1.csv' and 'Nacionality' in X.columns:\n",
    "        print(f\"Eliminating 'Nacionality' feature from {dataset_name}.\")\n",
    "        X = X.drop('Nacionality', axis=1)\n",
    "\n",
    "    #list of sensitive features\n",
    "    sensitive_features_list = ['Marital status', 'Application mode', 'Course',\n",
    "                                'Previous qualification', 'Nacionality',\n",
    "                               \"Mother's qualification\",\n",
    "                               'Educational special needs',\n",
    "                               'Tuition fees up to date', 'Gender',\n",
    "                               'Age at enrollment', 'International']\n",
    "    if 'Nacionality' not in X.columns and 'Nacionality' in sensitive_features_list:\n",
    "        sensitive_features_list.remove('Nacionality')\n",
    "\n",
    "\n",
    "    # Identify categorical and numerical features\n",
    "    categorical_features = X.select_dtypes(include=['object']).columns.difference(sensitive_features_list)\n",
    "    numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.difference(sensitive_features_list)\n",
    "    numerical_transformer = StandardScaler()\n",
    "    categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ], remainder='passthrough') # Keep sensitive features in X for evaluation\n",
    "\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    #full model pipeline\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"Training the classification model for {dataset_name}...\")\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    print(f\"Model training complete for {dataset_name}.\\n\")\n",
    "\n",
    "    #Model Evaluation\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "    print(f\"--- Model Performance Evaluation for {dataset_name} ---\")\n",
    "    # Print accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Print the classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 35 + \"\\n\")\n",
    "\n",
    "\n",
    "    #Fairness Evaluation (One-vs-Rest)\n",
    "\n",
    "    unique_classes = y.unique()\n",
    "    dpd_results = {}\n",
    "    eod_results = {}\n",
    "\n",
    "\n",
    "    for sensitive_feature in sensitive_features_list:\n",
    "        if sensitive_feature not in X_test.columns:\n",
    "            print(f\"Warning: Sensitive feature '{sensitive_feature}' not found in the test set for {dataset_name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        sensitive_test = X_test[sensitive_feature]\n",
    "\n",
    "        print(f\"\\n--- Fairness Evaluation (by '{sensitive_feature}' - One-vs-Rest) for {dataset_name} ---\")\n",
    "\n",
    "        dpd_results[sensitive_feature] = {}\n",
    "        eod_results[sensitive_feature] = {}\n",
    "\n",
    "        for target_class in unique_classes:\n",
    "            print(f\"\\nEvaluating fairness for class: '{target_class}' (One-vs-Rest) for {dataset_name}\")\n",
    "\n",
    "            y_test_binary = (y_test == target_class).astype(int)\n",
    "            y_pred_binary = (y_pred == target_class).astype(int)\n",
    "            metrics_binary = {\n",
    "                'accuracy': accuracy_score,\n",
    "                'precision': lambda y_true_b, y_pred_b: classification_report(y_true_b, y_pred_b, output_dict=True, zero_division=0)['weighted avg']['precision'],\n",
    "                'recall': lambda y_true_b, y_pred_b: classification_report(y_true_b, y_pred_b, output_dict=True, zero_division=0)['weighted avg']['recall']\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                grouped_on_sensitive = MetricFrame(metrics=metrics_binary,\n",
    "                                                       y_true=y_test_binary,\n",
    "                                                       y_pred=y_pred_binary,\n",
    "                                                       sensitive_features=sensitive_test)\n",
    "\n",
    "                print(f\"Metrics grouped by '{sensitive_feature}':\")\n",
    "                print(grouped_on_sensitive.by_group)\n",
    "\n",
    "                #fairness metrics\n",
    "                print(\"Fairness Metrics for this class:\")\n",
    "                dpd_binary = demographic_parity_difference(y_test_binary, y_pred_binary, sensitive_features=sensitive_test)\n",
    "                print(f\"Demographic Parity Difference: {dpd_binary:.4f}\")\n",
    "                dpd_results[sensitive_feature][target_class] = dpd_binary\n",
    "\n",
    "\n",
    "                eod_binary = equalized_odds_difference(y_test_binary, y_pred_binary, sensitive_features=sensitive_test)\n",
    "                print(f\"Equalized Odds Difference: {eod_binary:.4f}\")\n",
    "                eod_results[sensitive_feature][target_class] = eod_binary\n",
    "\n",
    "                print(\"-\" * 35)\n",
    "\n",
    "            except ValueError as e:\n",
    "                print(f\"Could not calculate fairness metrics for class '{target_class}' with sensitive feature '{sensitive_feature}' for {dataset_name}: {e}\")\n",
    "                print(\"This might happen if one of the sensitive feature groups has no samples for this class in the test set.\")\n",
    "                print(\"-\" * 35)\n",
    "\n",
    "    return pd.DataFrame(dpd_results).T, pd.DataFrame(eod_results).T\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to load data, train a model, and evaluate its\n",
    "    performance and fairness for both datasets.\n",
    "    \"\"\"\n",
    "    global dpd_results_data1\n",
    "    global eod_results_data1\n",
    "    global dpd_results_data2\n",
    "    global eod_results_data2\n",
    "    try:\n",
    "        df2 = pd.read_csv('data2 - Sheet1.csv')\n",
    "        print(\"\\nDataFrame columns for data2 - Sheet1.csv:\", df2.columns)\n",
    "        dpd_results_data2, eod_results_data2 = perform_fairness_evaluation(df2, 'data2 - Sheet1.csv')\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'data2 - Sheet1.csv' not found. Make sure the dataset file is in the same directory.\")\n",
    "        dpd_results_data2, eod_results_data2 = pd.DataFrame(), pd.DataFrame() # Assign empty dataframes on error\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "#Comparative fairness evaluation\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Identify features with potentially significant bias (e.g., DPD or EOD > 0.1)\n",
    "bias_threshold = 0.1\n",
    "\n",
    "print(f\"Features with potentially significant bias (DPD or EOD > {bias_threshold}):\")\n",
    "\n",
    "# Analyze results for data.csv\n",
    "print(\"\\n--- Bias Analysis for data.csv ---\")\n",
    "biased_features_data1 = set()\n",
    "if 'dpd_results_data1' in globals() and not dpd_results_data1.empty:\n",
    "    print(\"\\nAnalyzing Demographic Parity Difference (DPD) for data.csv:\")\n",
    "    for feature in dpd_results_data1.index:\n",
    "        if any(dpd_results_data1.loc[feature] > bias_threshold):\n",
    "            biased_features_data1.add(feature)\n",
    "            print(f\"- {feature} (DPD bias detected for classes: {[cls for cls, val in dpd_results_data1.loc[feature].items() if val > bias_threshold]})\")\n",
    "else:\n",
    "    print(\"Fairness evaluation results for data.csv not found or empty.\")\n",
    "\n",
    "if 'eod_results_data1' in globals() and not eod_results_data1.empty:\n",
    "    print(\"\\nAnalyzing Equalized Odds Difference (EOD) for data.csv:\")\n",
    "    for feature in eod_results_data1.index:\n",
    "        if any(eod_results_data1.loc[feature] > bias_threshold):\n",
    "            biased_features_data1.add(feature)\n",
    "            print(f\"- {feature} (EOD bias detected for classes: {[cls for cls, val in eod_results_data1.loc[feature].items() if val > bias_threshold]})\")\n",
    "else:\n",
    "    print(\"Fairness evaluation results for data.csv not found or empty.\")\n",
    "\n",
    "if not biased_features_data1:\n",
    "    print(\"\\nNo features found with bias above the threshold for data.csv.\")\n",
    "else:\n",
    "    print(\"\\nSummary of features with potentially significant bias for data.csv:\")\n",
    "    for feature in biased_features_data1:\n",
    "        print(f\"- {feature}\")\n",
    "\n",
    "# results for data2 - Sheet1.csv\n",
    "print(\"\\n--- Bias Analysis for data2 - Sheet1.csv ---\")\n",
    "biased_features_data2 = set()\n",
    "if 'dpd_results_data2' in globals() and not dpd_results_data2.empty:\n",
    "    print(\"\\nAnalyzing Demographic Parity Difference (DPD) for data2 - Sheet1.csv:\")\n",
    "    for feature in dpd_results_data2.index:\n",
    "        if any(dpd_results_data2.loc[feature] > bias_threshold):\n",
    "            biased_features_data2.add(feature)\n",
    "            print(f\"- {feature} (DPD bias detected for classes: {[cls for cls, val in dpd_results_data2.loc[feature].items() if val > bias_threshold]})\")\n",
    "else:\n",
    "    print(\"Fairness evaluation results for data2 - Sheet1.csv not found or empty.\")\n",
    "\n",
    "if 'eod_results_data2' in globals() and not eod_results_data2.empty:\n",
    "    print(\"\\nAnalyzing Equalized Odds Difference (EOD) for data2 - Sheet1.csv:\")\n",
    "    for feature in eod_results_data2.index:\n",
    "        if any(eod_results_data2.loc[feature] > bias_threshold):\n",
    "            biased_features_data2.add(feature)\n",
    "            print(f\"- {feature} (EOD bias detected for classes: {[cls for cls, val in eod_results_data2.loc[feature].items() if val > bias_threshold]})\")\n",
    "else:\n",
    "    print(\"Fairness evaluation results for data2 - Sheet1.csv not found or empty.\")\n",
    "\n",
    "if not biased_features_data2:\n",
    "    print(\"\\nNo features found with bias above the threshold for data2 - Sheet1.csv.\")\n",
    "else:\n",
    "    print(\"\\nSummary of features with potentially significant bias for data2 - Sheet1.csv:\")\n",
    "    for feature in biased_features_data2:\n",
    "        print(f\"- {feature}\")\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "if 'dpd_results_data1' not in globals() or 'eod_results_data1' not in globals() or \\\n",
    "   'dpd_results_data2' not in globals() or 'eod_results_data2' not in globals() or \\\n",
    "   dpd_results_data1.empty or eod_results_data1.empty or dpd_results_data2.empty or eod_results_data2.empty:\n",
    "    print(\"Fairness evaluation results for both datasets are required. Please run the fairness evaluation cell first.\")\n",
    "else:\n",
    "    # Calculate the difference in DPD and EOD between the two datasets\n",
    "    dpd_diff = dpd_results_data2.align(dpd_results_data1, join='outer', axis=0)[0] - dpd_results_data2.align(dpd_results_data1, join='outer', axis=0)[1]\n",
    "    eod_diff = eod_results_data2.align(eod_results_data1, join='outer', axis=0)[0] - eod_results_data2.align(eod_results_data1, join='outer', axis=0)[1]\n",
    "\n",
    "    # Fill NaN values\n",
    "    dpd_diff = dpd_diff.fillna(0)\n",
    "    eod_diff = eod_diff.fillna(0)\n",
    "\n",
    "\n",
    "    print(\"Difference in Demographic Parity Difference (data2 - data1):\")\n",
    "    display(dpd_diff)\n",
    "\n",
    "    print(\"\\nDifference in Equalized Odds Difference (data2 - data1):\")\n",
    "    display(eod_diff)\n",
    "\n",
    "\n",
    "    # Visualize the differences\n",
    "    dpd_diff_melted = dpd_diff.reset_index().melt(id_vars='index', var_name='Target Class', value_name='DPD Difference')\n",
    "    dpd_diff_melted = dpd_diff_melted.rename(columns={'index': 'Sensitive Feature'})\n",
    "\n",
    "    eod_diff_melted = eod_diff.reset_index().melt(id_vars='index', var_name='Target Class', value_name='EOD Difference')\n",
    "    eod_diff_melted = eod_diff_melted.rename(columns={'index': 'Sensitive Feature'})\n",
    "\n",
    "    fig_dpd = px.bar(dpd_diff_melted, x='Sensitive Feature', y='DPD Difference', color='Target Class',\n",
    "                     title='Difference in Demographic Parity Difference (data2 - data1) by Sensitive Feature and Target Class',\n",
    "                     barmode='group')\n",
    "    fig_dpd.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "    fig_dpd.show()\n",
    "\n",
    "    fig_eod = px.bar(eod_diff_melted, x='Sensitive Feature', y='EOD Difference', color='Target Class',\n",
    "                     title='Difference in Equalized Odds Difference (data2 - data1) by Sensitive Feature and Target Class',\n",
    "                     barmode='group')\n",
    "    fig_eod.update_layout(xaxis={'categoryorder':'total descending'})\n",
    "    fig_eod.show()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "if 'dpd_diff' not in globals() or 'eod_diff' not in globals() or dpd_diff.empty or eod_diff.empty:\n",
    "    print(\"Difference dataframes (dpd_diff, eod_diff) not found or are empty. Please run the previous cell to calculate them.\")\n",
    "else:\n",
    "    print(\"--- Analysis of Bias Differences (data2 - data1) ---\")\n",
    "\n",
    "    # Analyze DPD differences\n",
    "    print(\"\\nAnalyzing Demographic Parity Difference (DPD) changes:\")\n",
    "    if not dpd_diff.empty:\n",
    "\n",
    "        max_dpd_diff = dpd_diff.abs().max().max()\n",
    "        if max_dpd_diff > 0:\n",
    "\n",
    "            max_dpd_feature, max_dpd_class = dpd_diff.abs().stack().idxmax()\n",
    "            max_dpd_value = dpd_diff.loc[max_dpd_feature, max_dpd_class]\n",
    "            change_direction = \"increase\" if max_dpd_value > 0 else \"decrease\"\n",
    "            print(f\"The largest absolute DPD change ({abs(max_dpd_value):.4f}) is for '{max_dpd_feature}' in the '{max_dpd_class}' class. The bias {change_direction}d in data2 compared to data1.\")\n",
    "\n",
    "            # Identify features with notable DPD changes (here absolute difference > 0.2)\n",
    "            notable_dpd_changes = dpd_diff.stack()[(dpd_diff.abs().stack() > 0.2)].sort_values(ascending=False)\n",
    "            if not notable_dpd_changes.empty:\n",
    "                print(\"\\nSensitive features and target classes with notable DPD changes (absolute difference > 0.2):\")\n",
    "                for (feature, target_class), value in notable_dpd_changes.items():\n",
    "                    change_direction = \"increased\" if value > 0 else \"decreased\"\n",
    "                    print(f\"- '{feature}' ('{target_class}' class): Change of {value:.4f}. Bias {change_direction}.\")\n",
    "            else:\n",
    "                print(\"\\nNo features show a notable DPD change (absolute difference > 0.2).\")\n",
    "\n",
    "        else:\n",
    "            print(\"No change in DPD observed between the datasets.\")\n",
    "    else:\n",
    "        print(\"DPD difference dataframe is empty.\")\n",
    "\n",
    "\n",
    "    # Analyze EOD differences\n",
    "    print(\"\\nAnalyzing Equalized Odds Difference (EOD) changes:\")\n",
    "    if not eod_diff.empty:\n",
    "\n",
    "\n",
    "        max_eod_diff = eod_diff.abs().max().max()\n",
    "        if max_eod_diff > 0:\n",
    "            max_eod_feature, max_eod_class = eod_diff.abs().stack().idxmax()\n",
    "            max_eod_value = eod_diff.loc[max_eod_feature, max_eod_class]\n",
    "            change_direction = \"increase\" if max_eod_value > 0 else \"decrease\"\n",
    "            print(f\"The largest absolute EOD change ({abs(max_eod_value):.4f}) is for '{max_eod_feature}' in the '{max_eod_class}' class. The bias {change_direction}d in data2 compared to data1.\")\n",
    "\n",
    "            # Identify features with notable EOD changes (here absolute difference > 0.2)\n",
    "            notable_eod_changes = eod_diff.stack()[(eod_diff.abs().stack() > 0.2)].sort_values(ascending=False)\n",
    "            if not notable_eod_changes.empty:\n",
    "                print(\"\\nSensitive features and target classes with notable EOD changes (absolute difference > 0.2):\")\n",
    "                for (feature, target_class), value in notable_eod_changes.items():\n",
    "                    change_direction = \"increased\" if value > 0 else \"decreased\"\n",
    "                    print(f\"- '{feature}' ('{target_class}' class): Change of {value:.4f}. Bias {change_direction}.\")\n",
    "            else:\n",
    "                 print(\"\\nNo features show a notable EOD change (absolute difference > 0.2).\")\n",
    "        else:\n",
    "             print(\"No change in EOD observed between the datasets.\")\n",
    "    else:\n",
    "        print(\"EOD difference dataframe is empty.\")\n",
    "\n",
    "# Task\n",
    "Generate a Flask API that takes data as input, preprocesses it using the previously trained preprocessor, and returns predictions using the previously trained model. The API should be deployable and accessible from a web application.\n",
    "\n",
    "## Setup flask and dependencies\n",
    "\n",
    "### Subtask:\n",
    "Install necessary libraries like Flask and gunicorn.\n",
    "\n",
    "\n",
    "**Reasoning**:\n",
    "Install Flask and gunicorn using pip.\n",
    "\n",
    "\n",
    "\n",
    "%pip install Flask gunicorn\n",
    "\n",
    "## Save the trained model and preprocessor\n",
    "\n",
    "### Subtask:\n",
    "Save the trained model pipeline and the preprocessor to disk so they can be loaded by the API.\n",
    "\n",
    "\n",
    "%pip install fairlearn\n",
    "\n",
    "**Reasoning**:\n",
    "Now that fairlearn is installed, I can re-run the code to train the model and preprocessor and save them to disk.\n",
    "\n",
    "\n",
    "\n",
    "!gdown 1dZT7wMzJqRPzNRSNpcBcpf59wEc33Nd3\n",
    "\n",
    "**Reasoning**:\n",
    "Now that the data file is downloaded, I can re-run the code to load the data, train the model and preprocessor, and then save them to disk.\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from fairlearn.metrics import MetricFrame, demographic_parity_difference, equalized_odds_difference\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('data.csv', sep=';', quotechar='\"')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'data.csv' not found. Make sure the dataset file is in the same directory.\")\n",
    "    raise FileNotFoundError(\"data.csv not found\")\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "# The list of sensitive features to evaluate (used for fairness metrics, not dropped)\n",
    "sensitive_features_list = ['Marital status', 'Application mode', 'Course',\n",
    "                           'Previous qualification', 'Nacionality',\n",
    "                           \"Mother's qualification\",\n",
    "                           'Educational special needs',\n",
    "                           'Tuition fees up to date', 'Gender',\n",
    "                           'Age at enrollment', 'International']\n",
    "\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "# Exclude sensitive features from the features to be transformed\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.difference(sensitive_features_list)\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns.difference(sensitive_features_list)\n",
    "\n",
    "# Create transformers for numerical and categorical features\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Create a column transformer to apply different transformations to different columns\n",
    "# 'remainder='passthrough'' keeps the columns not specified in the transformers,\n",
    "# which includes the sensitive features that we need in X_test for fairness evaluation.\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ], remainder='passthrough')\n",
    "\n",
    "\n",
    "# Model Training\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Create the full model pipeline including preprocessing and the classifier\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the classification model...\")\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Save the trained model pipeline\n",
    "joblib.dump(model_pipeline, 'model_pipeline.joblib')\n",
    "\n",
    "# Save the trained preprocessor\n",
    "joblib.dump(preprocessor, 'preprocessor.joblib')\n",
    "\n",
    "print(\"Model pipeline and preprocessor saved successfully.\")\n",
    "\n",
    "## Create the flask application\n",
    "\n",
    "### Subtask:\n",
    "Write the code for the Flask app, including loading the saved model and preprocessor.\n",
    "\n",
    "\n",
    "**Reasoning**:\n",
    "Write the Flask application code to load the model and preprocessor and define the necessary routes.\n",
    "\n",
    "\n",
    "\n",
    "!pip install flask_cors pyngrok\n",
    "\n",
    "import os\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from flask_cors import CORS # Import CORS\n",
    "from pyngrok import ngrok\n",
    "import threading\n",
    "from google.colab import userdata\n",
    "\n",
    "# Define the path to the saved model and preprocessor\n",
    "MODEL_PATH = 'model_pipeline.joblib'\n",
    "PREPROCESSOR_PATH = 'preprocessor.joblib'\n",
    "\n",
    "# Load the saved model and preprocessor\n",
    "try:\n",
    "    model_pipeline = joblib.load(MODEL_PATH)\n",
    "    preprocessor = joblib.load(PREPROCESSOR_PATH)\n",
    "    print(\"Model pipeline and preprocessor loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model or preprocessor file not found. Ensure '{MODEL_PATH}' and '{PREPROCESSOR_PATH}' exist.\")\n",
    "    exit() # Exit if files are not found\n",
    "\n",
    "# Initialize the Flask application\n",
    "app = Flask(__name__)\n",
    "CORS(app) # Enable CORS for all origins\n",
    "\n",
    "# Define the root route\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \"Flask app is running\"\n",
    "\n",
    "# Define the prediction route\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Get data from the request\n",
    "        data = request.get_json()\n",
    "\n",
    "        # Convert the incoming JSON data to a pandas DataFrame\n",
    "        # Assuming the JSON data is a list of dictionaries, where each dictionary is a data point\n",
    "        input_df = pd.DataFrame(data)\n",
    "\n",
    "        # Ensure the input DataFrame has the same columns as the training data\n",
    "        # This is a basic check; a more robust solution would handle missing/extra columns\n",
    "        # based on the columns used during training. For simplicity, we'll assume\n",
    "        # the input data matches the original training data columns before preprocessing.\n",
    "        # You would typically get the original column names from your training data\n",
    "        # or save them during the training phase.\n",
    "        # For now, we'll rely on the preprocessor's remainder='passthrough' to handle\n",
    "        # columns not explicitly transformed, assuming sensitive features are present\n",
    "        # and other unexpected columns are passed through (though this might not be\n",
    "        # the desired behavior). A better approach would align columns with X_train.columns.\n",
    "\n",
    "        # Since the preprocessor was fitted on X_train which included sensitive features\n",
    "        # with remainder='passthrough', the input_df *must* contain all columns\n",
    "        # present in the original training DataFrame (X). Let's load the original\n",
    "        # dataframe columns to ensure alignment.\n",
    "        try:\n",
    "            original_df_columns = pd.read_csv('data.csv', sep=';', quotechar='\"').drop('Target', axis=1).columns.tolist()\n",
    "            # Reindex the input_df to match the original columns.\n",
    "            # This will add missing columns with NaN and drop extra columns.\n",
    "            # You might need to handle NaNs appropriately based on your data.\n",
    "            input_df = input_df.reindex(columns=original_df_columns, fill_value=None) # Use None or a suitable default\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             print(\"Error: 'data.csv' not found. Cannot align input data columns.\")\n",
    "             return jsonify({'error': 'Internal server error: Original data columns not found.'}), 500\n",
    "        except Exception as e:\n",
    "             print(f\"Error aligning input data columns: {e}\")\n",
    "             return jsonify({'error': f'Internal server error: Could not align input data columns: {e}'}), 500\n",
    "\n",
    "\n",
    "        # Preprocess the input data using the loaded preprocessor\n",
    "        processed_data = preprocessor.transform(input_df)\n",
    "\n",
    "        # Make predictions using the loaded model pipeline\n",
    "        predictions = model_pipeline.predict(processed_data)\n",
    "\n",
    "        # Convert predictions to a list and return as JSON\n",
    "        return jsonify(predictions.tolist())\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error for debugging\n",
    "        app.logger.error('An error occurred during prediction: %s', e)\n",
    "        return jsonify({'error': 'An internal error occurred. Please try again later.'}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Get ngrok authtoken from Colab secrets\n",
    "    NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n",
    "\n",
    "    # Set ngrok authtoken\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "    # Start ngrok tunnel\n",
    "    public_url = ngrok.connect(5000).public_url\n",
    "    print(f\" * ngrok tunnel is live at {public_url}\")\n",
    "\n",
    "    # Start Flask app in a separate thread\n",
    "    threading.Thread(target=app.run, kwargs={'host': '0.0.0.0', 'port': 5000, 'debug': True, 'use_reloader': False}).start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
